# 實驗記錄
[paper note](https://hackmd.io/T4tBBaITQQqLvDgUw3HWvA)

## Algorithm
![](https://i.imgur.com/1jmPWJr.png)

## Softmax (Cross-Entropy) Training
```
python softmax_training.py
```
![](https://i.imgur.com/jmGG0GG.png)

## Prototype Conformity Loss
```
python pcl_training.py
```
## Adversarial Training
```
python pcl_training_adversarial_pgd.py
```
![](https://i.imgur.com/B4RYli4.png)

## Testing Model's Robustness against White-Box Attacks
```
python robustness.py
```
![](https://i.imgur.com/5I1BrXF.png)

## Comparison of Softmax Trained Model and Our Model
![](https://i.imgur.com/LdGZH9I.png)

### attack method
以下兩種都是對FGSM進行迭代
#### BIM
#### PGD [Towards Deep Learning Models Resistant to Adversarial Attacks](https://arxiv.org/pdf/1706.06083.pdf)
- 比BIM 多了對擾動的隨機處理。
- 在MNIST、CIFAR-10實現發現PGD要比BIM的攻擊效果好很多。
> 他們的PGD攻擊包括在允許的範數球內的隨機點初始化搜索對抗示例，然後運行基本迭代方法的多次迭代（Kurakin等人，2017b）以找到對抗示例。 嘈雜的起始點會比其他先前的迭代方法（例如BIM）產生更強的攻擊力（Kurakin等人，2017a），並且在這種更強的攻擊力下進行對抗訓練會使他們的防禦更加成功（Madry等人，2017）。
> 

[reference](https://zhuanlan.zhihu.com/p/45684812)


---

### ShieldNets: Defending Against Adversarial Attacks Using Probabilistic Adversarial Robustness (CVPR2019)
防禦對抗攻擊是為工業應用可靠部署深度學習授權解決方案的關鍵一步。通過將概率概率魯棒性（PAR）作為理論框架，通過將樣本概率集中到無對抗區域來抵消對抗攻擊。與大多數現有的防禦機制不同，**後者需要修改目標分類器的體系結構/訓練，而在現實情況下這是不可行的，例如，在已經部署模型的情況下，PAR首先設計為為現有的固定模型提供主動保護。** ShieldNet通過使用PixelCNN來實現PAR的演示。實驗結果表明，該方法是通用的，對對抗傳遞性具有魯棒性，並且可以分別抵抗對Fashion-MNIST和CIFAR10數據集的各種攻擊。

### Barrage of Random Transforms for Adversarially Robust Defense (CVPR2019)
使用ImageNet數據集時，針對對抗性示例的防禦在歷史上很容易被擊敗。 普遍的理解是，當考慮到模糊梯度時，簡單的圖像變換和其他各種防禦措施的組合不足以提供必要的保護。 在本文中，我們 **探索了將大量弱勢防禦隨機組合到單個隨機變換彈幕中的構想，以構建強大的防禦對抗性攻擊。** 我們證明，即使考慮了混淆的梯度，隨機變換彈幕（BaRT）仍然可以抵禦最困難的攻擊，例如PGD。 與以前的工作相比，BaRT的準確度提高了24％，甚至將有效性擴展到了未經測試的最大對抗性擾動✏= 32。